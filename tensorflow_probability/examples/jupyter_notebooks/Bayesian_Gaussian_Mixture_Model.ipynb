{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bayesian_Gaussian_Mixture_Model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "JJ3UDciDVcB5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Bayesian Gaussian Mixture Model and Hamiltonian MCMC\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/probability/blob/notebooks/tensorflow_probability/examples/jupyter_notebooks/Bayesian_Gaussian_Mixture_Model.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/probability/blob/notebooks/tensorflow_probability/examples/jupyter_notebooks/Bayesian_Gaussian_Mixture_Model.ipynb#scrollTo=JJ3UDciDVcB5\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "irUl_YaryhRf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "975e92f6-3b08-4f4f-9dc4-13502f2bda47"
      },
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_probability"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow_probability in /usr/local/lib/python3.6/dist-packages (0.3.0)\r\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_probability) (1.11.0)\r\n",
            "Requirement already satisfied: tensorflow>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_probability) (1.10.0)\n",
            "Requirement already satisfied: numpy<=1.14.5,>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow_probability) (1.14.5)\n",
            "Requirement already satisfied: setuptools<=39.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10.0->tensorflow_probability) (39.1.0)\n",
            "Requirement already satisfied: tensorboard<1.11.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10.0->tensorflow_probability) (1.10.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10.0->tensorflow_probability) (0.2.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10.0->tensorflow_probability) (0.7.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10.0->tensorflow_probability) (0.31.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10.0->tensorflow_probability) (3.6.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10.0->tensorflow_probability) (1.14.1)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10.0->tensorflow_probability) (0.4.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10.0->tensorflow_probability) (1.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.11.0,>=1.10.0->tensorflow>=1.10.0->tensorflow_probability) (0.14.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.11.0,>=1.10.0->tensorflow>=1.10.0->tensorflow_probability) (2.6.11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-iF7xFWVyKxV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this colab we'll explore sampling from the posterior of a Bayesian Gaussian Mixture Model (BGMM) using only TensorFlow Probability primitives."
      ]
    },
    {
      "metadata": {
        "id": "eZs1ShikNBK2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model"
      ]
    },
    {
      "metadata": {
        "id": "7JjokKMbk2hJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For $k\\in\\{1,\\ldots, K\\}$ mixture components each of dimension $D$, we'd like to model $i\\in\\{1,\\ldots,N\\}$ iid samples using the following Bayesian Gaussian Mixture Model:\n",
        "\n",
        "$$\\begin{align*}\n",
        "\\theta &\\sim \\text{Dirichlet}(\\text{concentration}=\\alpha_0)\\\\\n",
        "\\mu_k &\\sim \\text{Normal}(\\text{loc}=\\mu_{0k}, \\text{scale}=I_D)\\\\\n",
        "T_k &\\sim \\text{Wishart}(\\text{df}=5, \\text{scale}=I_D)\\\\\n",
        "Z_i &\\sim \\text{Categorical}(\\text{probs}=\\theta)\\\\\n",
        "Y_i &\\sim \\text{Normal}(\\text{loc}=\\mu_{z_i}, \\text{scale}=T_{z_i}^{-1/2})\\\\\n",
        "\\end{align*}$$"
      ]
    },
    {
      "metadata": {
        "id": "iySRABi0qZnQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Note, the `scale` arguments all have `cholesky` semantics. We use this convention because it is that of TF Distributions (which itself uses this convention in part because it is computationally advantageous)."
      ]
    },
    {
      "metadata": {
        "id": "Y6X_Beihwzyi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Our goal is to generate samples from the posterior:\n",
        "\n",
        "$$p\\left(\\theta, \\{\\mu_k, T_k\\}_{k=1}^K \\Big| \\{y_i\\}_{i=1}^N, \\alpha_0, \\{\\mu_{ok}\\}_{k=1}^K\\right)$$\n",
        "\n",
        "Notice that $\\{Z_i\\}_{i=1}^N$ is not present--we're interested in only those random variables which don't scale with $N$.  (And luckily there's a TF distribution which handles marginalizing out $Z_i$.)\n",
        "\n",
        "It is not possible to directly sample from this distribution owing to a computationally intractable normalization term.\n",
        "\n",
        "[Metropolis-Hastings algorithms](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm) are technique for for sampling from intractable-to-normalize distributions.\n",
        "\n",
        "TensorFlow Probability offers a number of MCMC options, including several based on Metropolis-Hastings. In this notebook, we'll use [Hamiltonian Monte Carlo](https://en.wikipedia.org/wiki/Hamiltonian_Monte_Carlo)  (`tfp.mcmc.HamiltonianMonteCarlo`). HMC is often a good choice because it can converge rapidly, samples the state space jointly (as opposed to coordinatewise), and leverages one of TF's virtues: automatic differentiation. That said, sampling from a BGMM posterior might actually be better done by other approaches, e.g., [Gibb's sampling](https://en.wikipedia.org/wiki/Gibbs_sampling)."
      ]
    },
    {
      "metadata": {
        "id": "uswTWdgNu46j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import matplotlib.pyplot as plt; plt.style.use('ggplot')\n",
        "import numpy as np\n",
        "import seaborn as sns; sns.set_context('notebook')\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "from tensorflow.python.ops.distributions import util as distribution_util\n",
        "\n",
        "tfd = tfp.distributions\n",
        "tfb = tfp.bijectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ovNsKD-OEUzR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def session_options(enable_gpu_ram_resizing=True):\n",
        "  \"\"\"Convenience function which sets common `tf.Session` options.\"\"\"\n",
        "  config = tf.ConfigProto()\n",
        "  config.log_device_placement = True\n",
        "  if enable_gpu_ram_resizing:\n",
        "    # `allow_growth=True` makes it possible to connect multiple colabs to your\n",
        "    # GPU. Otherwise the colab malloc's all GPU ram.\n",
        "    config.gpu_options.allow_growth = True\n",
        "  return config\n",
        "\n",
        "def reset_sess(config=None):\n",
        "  \"\"\"Convenience function to create the TF graph and session, or reset them.\"\"\"\n",
        "  if config is None:\n",
        "    config = session_options()\n",
        "  tf.reset_default_graph()\n",
        "  global sess\n",
        "  try:\n",
        "    sess.close()\n",
        "  except:\n",
        "    pass\n",
        "  sess = tf.InteractiveSession(config=config)\n",
        "\n",
        "reset_sess()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Uj9uHZN2yUqz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Before actually building the model, we'll need to define a new type of distribution.  From the model specification above, its clear we're parameterizing the MVN with an inverse covariance matrix, i.e.,  [precision matrix](https://en.wikipedia.org/wiki/Precision_(statistics%29).  To accomplish this in TF,  we'll need to roll out our `Bijector`.  This `Bijector` will use the forward transformation:\n",
        "\n",
        "- `Y =`  [`tf.matrix_triangular_solve`](https://www.tensorflow.org/api_docs/python/tf/matrix_triangular_solve)`(tf.matrix_transpose(chol_precision_tril), X, adjoint=True) + loc`.\n",
        "\n",
        "And the `log_prob` calculation is just the inverse, i.e.:\n",
        "\n",
        "- `X =` [`tf.matmul`](https://www.tensorflow.org/api_docs/python/tf/matmul)`(chol_precision_tril, X - loc, adjoint_a=True)`.\n",
        "\n",
        "Since all we need for HMC is `log_prob`, this means we avoid ever calling `tf.matrix_triangular_solve` (as would be the case for `tfd.MultivariateNormalTriL`). This is advantageous since `tf.matmul` is usually faster owing to better cache locality.\n"
      ]
    },
    {
      "metadata": {
        "id": "nc4yy6vW-lC_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MVNCholPrecisionTriL(tfd.TransformedDistribution):\n",
        "  \"\"\"MVN from loc and (Cholesky) precision matrix.\"\"\"\n",
        "\n",
        "  def __init__(self, loc, chol_precision_tril, name=None):\n",
        "    super(MVNCholPrecisionTriL, self).__init__(\n",
        "        distribution=tfd.Independent(tfd.Normal(tf.zeros_like(loc),\n",
        "                                                scale=tf.ones_like(loc)),\n",
        "                                     reinterpreted_batch_ndims=1),\n",
        "        bijector=tfb.Chain([\n",
        "            tfb.Affine(shift=loc),\n",
        "            tfb.Invert(tfb.Affine(scale_tril=chol_precision_tril,\n",
        "                                  adjoint=True)),\n",
        "        ]),\n",
        "        name=name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JDOkWhDQg4ZG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The `tfd.Independent` distribution turns independent draws of one distribution, into a multivariate distribution with statistically independent coordinates. In terms of computing `log_prob`, this \"meta-distribution\" manifests as a simple sum over the event dimension(s).\n",
        "\n",
        "Also notice that we took the `adjoint` (\"transpose\") of the scale matrix. This is because if precision is inverse covariance, i.e., $P=C^{-1}$ and if $C=AA^\\top$, then $P=BB^{\\top}$ where $B=A^{-\\top}$."
      ]
    },
    {
      "metadata": {
        "id": "Pfkc8cmhh2Qz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Since this distribution is kind of tricky, let's quickly verify that our `MVNCholPrecisionTriL` works as we think it should."
      ]
    },
    {
      "metadata": {
        "id": "GhqbjwlIh1Vn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "6d30bf3c-ac69-4cec-f7aa-55586137c481"
      },
      "cell_type": "code",
      "source": [
        "def compute_sample_stats(d, seed=42, n=int(1e6)):\n",
        "  x = d.sample(n, seed=seed)\n",
        "  sample_mean = tf.reduce_mean(x, axis=0, keepdims=True)\n",
        "  s = x - sample_mean\n",
        "  sample_cov = tf.matmul(s, s, adjoint_a=True) / tf.cast(n, s.dtype)\n",
        "  sample_scale = tf.cholesky(sample_cov)\n",
        "  sample_mean = sample_mean[0]\n",
        "  return [\n",
        "      sample_mean,\n",
        "      sample_cov,\n",
        "      sample_scale,\n",
        "  ]\n",
        "\n",
        "dtype = np.float32\n",
        "true_loc = np.array([1., -1.], dtype=dtype)\n",
        "true_chol_precision = np.array([[1., 0.],\n",
        "                                [2., 8.]],\n",
        "                               dtype=dtype)\n",
        "true_precision = np.matmul(true_chol_precision, true_chol_precision.T)\n",
        "true_cov = np.linalg.inv(true_precision)\n",
        "\n",
        "d = MVNCholPrecisionTriL(\n",
        "    loc=true_loc,\n",
        "    chol_precision_tril=true_chol_precision)\n",
        "\n",
        "[\n",
        "    sample_mean_,\n",
        "    sample_cov_,\n",
        "    sample_scale_,\n",
        "] = sess.run(compute_sample_stats(d))\n",
        "\n",
        "print('true mean:', true_loc)\n",
        "print('sample mean:', sample_mean_)\n",
        "print('true cov:\\n', true_cov)\n",
        "print('sample cov:\\n', sample_cov_)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "true mean: [ 1. -1.]\n",
            "sample mean: [ 1.0002806 -1.000105 ]\n",
            "true cov:\n",
            " [[ 1.0625   -0.03125 ]\n",
            " [-0.03125   0.015625]]\n",
            "sample cov:\n",
            " [[ 1.0641279  -0.03126174]\n",
            " [-0.03126174  0.01559311]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "N60z8scN1v6E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Since the sample mean and covariance are close to the true mean and covariance, it seems like the distribution is correctly implemented. Now, we'll use `MVNCholPrecisionTriL` and  stock`tfp.distributions` to specify the BGMM prior random variables:"
      ]
    },
    {
      "metadata": {
        "id": "xhzxySDjL2-S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dtype = np.float32\n",
        "dims = 2\n",
        "components = 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xAOmHhZ7LzDQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rv_mix_probs = tfd.Dirichlet(\n",
        "    concentration=np.ones(components, dtype) / 10.,\n",
        "    name='rv_mix_probs')\n",
        "\n",
        "rv_loc = tfd.Independent(\n",
        "    tfd.Normal(\n",
        "        loc=np.stack([\n",
        "            -np.ones(dims, dtype),\n",
        "            np.zeros(dims, dtype),\n",
        "            np.ones(dims, dtype),\n",
        "        ]),\n",
        "        scale=tf.ones([components, dims], dtype)),\n",
        "    reinterpreted_batch_ndims=1,\n",
        "    name='rv_loc')\n",
        "\n",
        "rv_precision = tfd.Wishart(\n",
        "    df=5,\n",
        "    scale_tril=np.stack([np.eye(dims, dtype=dtype)]*components),\n",
        "    input_output_cholesky=True,\n",
        "    name='rv_precision')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KSTp8aAIAv0O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "dc899498-a227-48bf-8d25-4186f8626279"
      },
      "cell_type": "code",
      "source": [
        "print(rv_mix_probs)\n",
        "print(rv_loc)\n",
        "print(rv_precision)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.distributions.Dirichlet(\"rv_mix_probs/\", batch_shape=(), event_shape=(3,), dtype=float32)\n",
            "tf.distributions.Independent(\"rv_loc/\", batch_shape=(3,), event_shape=(2,), dtype=float32)\n",
            "tf.distributions.Wishart(\"rv_precision/\", batch_shape=(3,), event_shape=(2, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8ZOG0OR815Nr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Using the three random variables defined above, we can now specify the joint log probability function. To do this we'll use `tfd.MixtureSameFamily` to automatically integrate out the categorical $\\{Z_i\\}_{i=1}^N$ draws."
      ]
    },
    {
      "metadata": {
        "id": "CpLnRJr2TXYD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def joint_log_prob(observations, mix_probs, loc, chol_precision):\n",
        "  \"\"\"BGMM with priors: loc=Normal, precision=Inverse-Wishart, mix=Dirichlet.\n",
        "\n",
        "  Args:\n",
        "    observations: `[n, d]`-shaped `Tensor` representing Bayesian Gaussian\n",
        "      Mixture model draws. Each sample is a length-`d` vector.\n",
        "    mix_probs: `[K]`-shaped `Tensor` representing random draw from\n",
        "      `SoftmaxInverse(Dirichlet)` prior.\n",
        "    loc: `[K, d]`-shaped `Tensor` representing the location parameter of the\n",
        "      `K` components.\n",
        "    chol_precision: `[K, d, d]`-shaped `Tensor` representing `K` lower\n",
        "      triangular `cholesky(Precision)` matrices, each being sampled from\n",
        "      a Wishart distribution.\n",
        "\n",
        "  Returns:\n",
        "    log_prob: `Tensor` representing joint log-density over all inputs.\n",
        "  \"\"\"\n",
        "  rv_observations = tfd.MixtureSameFamily(\n",
        "      mixture_distribution=tfd.Categorical(probs=mix_probs),\n",
        "      components_distribution=MVNCholPrecisionTriL(\n",
        "          loc=loc,\n",
        "          chol_precision_tril=chol_precision))\n",
        "  log_prob_parts = [\n",
        "      rv_observations.log_prob(observations), # Sum over samples.\n",
        "      rv_mix_probs.log_prob(mix_probs)[..., tf.newaxis],\n",
        "      rv_loc.log_prob(loc),                   # Sum over components.\n",
        "      rv_precision.log_prob(chol_precision),  # Sum over components.\n",
        "  ]\n",
        "  sum_log_prob = tf.reduce_sum(tf.concat(log_prob_parts, axis=-1), axis=-1)\n",
        "  # Note: for easy debugging, uncomment the following:\n",
        "  # sum_log_prob = tf.Print(sum_log_prob, log_prob_parts)\n",
        "  return sum_log_prob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QM1idLJazkGC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Notice that this function internally defines a new random variable. This is necessary since the `observations` RV depends on samples from the RVs defined further above."
      ]
    },
    {
      "metadata": {
        "id": "7jTMXdymV1QJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Generate \"Training\" Data"
      ]
    },
    {
      "metadata": {
        "id": "rl4brz3G3pS7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For this demo, we'll sample some random data."
      ]
    },
    {
      "metadata": {
        "id": "1AJZAtwXV8RQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_samples = 1000\n",
        "true_loc = np.array([[-2, -2],\n",
        "                     [0, 0],\n",
        "                     [2, 2]], dtype)\n",
        "random = np.random.RandomState(seed=42)\n",
        "\n",
        "true_hidden_component = random.randint(0, components, num_samples)\n",
        "observations = (true_loc[true_hidden_component] +\n",
        "                random.randn(num_samples, dims).astype(dtype))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zVOvMh7MV37A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Bayesian Inference using HMC"
      ]
    },
    {
      "metadata": {
        "id": "cdN3iKFT32Jp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now that we've used TFD to specify our model and obtained some observed data, we have all the necessary pieces to run HMC.\n",
        "\n",
        "To do this, we'll use a [closure](https://en.wikipedia.org/wiki/Closure_(computer_programming%29#Anonymous_functions) to \"pin down\" the things we don't want to sample. In this case that means we need only pin down `observations`. (The hyper-parameters are already baked in to the prior distributions and not part of the `joint_log_prob` function signature.)"
      ]
    },
    {
      "metadata": {
        "id": "tVoaDFSf7L_j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "unnormalized_posterior_log_prob = lambda *args: joint_log_prob(observations, *args)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a0OMIWIYeMmQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "initial_state = [\n",
        "    tf.fill([components],\n",
        "            value=np.array(1. / components, dtype),\n",
        "            name='mix_probs'),\n",
        "    tf.constant(np.array([[-2, -2],\n",
        "                          [0, 0],\n",
        "                          [2, 2]], dtype),\n",
        "                name='loc'),\n",
        "    tf.eye(dims, batch_shape=[components], dtype=dtype, name='chol_precision'),\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TVpiT3LLyfcO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Unconstrained Representation"
      ]
    },
    {
      "metadata": {
        "id": "JS8XOsxiyiBV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Hamiltonian Monte Carlo (HMC) requires the target log-probability function be differentiable with respect to its arguments.  Furthermore, HMC can exhibit dramatically higher statistical efficiency if the state-space is unconstrained.\n",
        "\n",
        "This means we'll have to work out two main issues when sampling from the BGMM posterior:\n",
        "\n",
        "1. $\\theta$ represents a discrete probability vector, i.e., must be such that $\\sum_{k=1}^K \\theta_k = 1$ and $\\theta_k>0$.\n",
        "2. $T_k$ represents an inverse covariance matrix, i.e., must be such that $T_k \\succ 0$, i.e., is [positive definite](https://en.wikipedia.org/wiki/Positive-definite_matrix).\n"
      ]
    },
    {
      "metadata": {
        "id": "Vt9SXJzO0Cks",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To address this requirement we'll need to:\n",
        "\n",
        "1. transform the constrained variables to an unconstrained space\n",
        "2. run the MCMC in unconstrained space\n",
        "3. transform the unconstrained variables back to the constrained space.\n",
        "\n",
        "As with `MVNCholPrecisionTriL`, we'll use [`Bijector`s](https://www.tensorflow.org/api_docs/python/tf/distributions/bijectors/Bijector) to transform random variables to unconstrained space.\n",
        "\n",
        "- The [`Dirichlet`](https://en.wikipedia.org/wiki/Dirichlet_distribution) is transformed to unconstrained space via the [softmax function](https://en.wikipedia.org/wiki/Softmax_function).\n",
        "\n",
        "- Our precision random variable is a distribution over postive semidefinite matrices. To unconstrain these we'll use the `FillTriangular` and `TransformDiagonal` bijectors.  These convert vectors to lower-triangular matrices and ensure the diagonal is positive. The former is useful because it enables sampling only $d(d+1)/2$ floats rather than $d^2$."
      ]
    },
    {
      "metadata": {
        "id": "_atEQrDR7JvG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "unconstraining_bijectors = [\n",
        "    tfb.SoftmaxCentered(),\n",
        "    tfb.Identity(),\n",
        "    tfb.Chain([\n",
        "        tfb.TransformDiagonal(tfb.Softplus()),\n",
        "        tfb.FillTriangular(),\n",
        "    ])]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0zq6QJJ-NSPJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "7cfff33d-c640-47d4-ddff-e9ce6935fbd8"
      },
      "cell_type": "code",
      "source": [
        "[mix_probs, loc, chol_precision], kernel_results = tfp.mcmc.sample_chain(\n",
        "    num_results=2000,\n",
        "    num_burnin_steps=500,\n",
        "    current_state=initial_state,\n",
        "    kernel=tfp.mcmc.TransformedTransitionKernel(\n",
        "        inner_kernel=tfp.mcmc.HamiltonianMonteCarlo(\n",
        "            target_log_prob_fn=unnormalized_posterior_log_prob,\n",
        "            step_size=0.065,\n",
        "            num_leapfrog_steps=5),\n",
        "        bijector=unconstraining_bijectors))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_probability/python/bijectors/softmax_centered.py:158: calling reduce_logsumexp (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_ceX1A3-ZFiN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "acceptance_rate = tf.reduce_mean(tf.to_float(kernel_results.inner_results.is_accepted))\n",
        "mean_mix_probs = tf.reduce_mean(mix_probs, axis=0)\n",
        "mean_loc = tf.reduce_mean(loc, axis=0)\n",
        "mean_chol_precision = tf.reduce_mean(chol_precision, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kmpTFZcVmByb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Note: through trial-and-error we've predetermined the `step_size` and `num_leapfrog_steps` to approximately achieve an [asymptotically optimal rate of 0.651](https://arxiv.org/abs/1001.4460). For a technique to do this automatically, see the examples section in `help(tfp.mcmc.HamiltonianMonteCarlo)`."
      ]
    },
    {
      "metadata": {
        "id": "QLEz96mg6fpZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We'll now execute the chain and print the posterior means."
      ]
    },
    {
      "metadata": {
        "id": "3B2yJWVmNcrm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "[\n",
        "    acceptance_rate_,\n",
        "    mean_mix_probs_,\n",
        "    mean_loc_,\n",
        "    mean_chol_precision_,\n",
        "    mix_probs_,\n",
        "    loc_,\n",
        "    chol_precision_,\n",
        "] = sess.run([\n",
        "    acceptance_rate,\n",
        "    mean_mix_probs,\n",
        "    mean_loc,\n",
        "    mean_chol_precision,\n",
        "    mix_probs,\n",
        "    loc,\n",
        "    chol_precision,\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bqJ6RSJxegC6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "8320b5b2-6da0-44d8-efab-fd40d609aabd"
      },
      "cell_type": "code",
      "source": [
        "print('    acceptance_rate:', acceptance_rate_)\n",
        "print('      avg mix probs:', mean_mix_probs_)\n",
        "print('\\n            avg loc:\\n', mean_loc_)\n",
        "print('\\navg chol(precision):\\n', mean_chol_precision_)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    acceptance_rate: 0.683\n",
            "      avg mix probs: [0.3906059  0.25069773 0.3586959 ]\n",
            "\n",
            "            avg loc:\n",
            " [[-1.8691756  -1.7867835 ]\n",
            " [-0.01069285  0.02102656]\n",
            " [ 1.8850509   1.9270732 ]]\n",
            "\n",
            "avg chol(precision):\n",
            " [[[ 1.0045815   0.        ]\n",
            "  [-0.06221696  0.9669791 ]]\n",
            "\n",
            " [[ 1.2323949   0.        ]\n",
            "  [ 0.24171357  1.0440764 ]]\n",
            "\n",
            " [[ 0.984149    0.        ]\n",
            "  [-0.11206173  0.96565163]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zFOU0j9kPdUy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "outputId": "0b440a64-8249-4631-ba7d-6e33ef86534f"
      },
      "cell_type": "code",
      "source": [
        "ax = sns.kdeplot(loc_[:,0,0], loc_[:,0,1], shade=True)\n",
        "ax = sns.kdeplot(loc_[:,1,0], loc_[:,1,1], shade=True)\n",
        "ax = sns.kdeplot(loc_[:,2,0], loc_[:,2,1], shade=True)\n",
        "plt.title('KDE of loc draws')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd8AAAFZCAYAAADZ6SWdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAMOILwAAhv0f9Yc3z5viAyEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f4860c0a9b0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "NmfNIM1c6mwc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Conclusion"
      ]
    },
    {
      "metadata": {
        "id": "t8LeIeMn6ot4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This simple colab demonstrated how TensorFlow Probability primitives can be used to build hierarchical Bayesian mixture models."
      ]
    }
  ]
}